---
title: "Deriving Ordinary Least Squares Estimators in Excruciating Detail (Part 1)"
author: "Erik Case"
runtime: shiny
output: html_document
---

```{r setup, include=FALSE}
library (reshape)
library(knitr)
library(ggvis)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
```

## Revisiting a Classic - Because Why?

Let's face it, finding the 'best' fit line via Ordinary Least Squares (OLS) just isn't sexy in today's world of more exotic methodological predictive techniques. Hell, the technique as we know it [is over 300 years old](http://www.stat.cmu.edu/~cshalizi/rmarkdown/rmarkdown.Rmd) Why devote my inagural post to something that is covered ad-nasueum in all baby Stats and even ML courses? 

My primary motivation for covering the derivation of $\hat\beta_0$ and $\hat\beta_1$ is to fill in some of the algebraic scaffolding left out of the online tutorials I have found found on this topic. Most people assume you are following every algebraic shortcut they make and I feel some people might benefit by seeing them derived in excruciating detail. I am also benefitting by reviewing the inner machinations of this clasdic technique.

The second reason is that OLS has a special place in my heart (and with many modelers) as my entree into the world of data science and applied statistics. I would have sworn up and down that I really wasn't a 'math person' until I was rudely awakened by a course called Multivariate Regression Analyis. Despite now being a data scientist I'm still not even a 'math person,' in so much as that it comes to me not by ease of intuition, but by plodding, deliberate study.  That

But OLS  made me question my aelf assesment of ineptitude turned me on to a career in statistics because it was *able* to draw me in was it's pure elegance and intuitive accessability to the non-expert. When the concept of a best fit line through a stochastic process was explained to me, it immediately and conceptually *made sense* , leaving me thinking "This is kind of fun. What if I could do this for a living?"



Take for example this totally made up and simulated plot of Ferbie Sales (in 1000's) and Violent Crime in the United States

```{r,echo=FALSE, warning=FALSE}
library(dplyr)
library(ggvis)

Ferbie_Sales<-rnorm(100) + 10

eps<-rnorm(100,0,3)

Violent_Crime<-as.numeric(50 + 5*Ferbie_Sales + eps)

require(reshape2)
d <- cbind(Violent_Crime, Ferbie_Sales)

d2<-as.data.frame(d)

d2 %>%
 ggvis(~Ferbie_Sales, ~ Violent_Crime, stroke:="blue") %>%
  layer_points()
```


Most people can instinctively intuit that there appears to be a positive relationship between Ferbie sales and violent crime. If we drew a solid line tracing this relationship we could actually measure the *magnitude* of the relationship. If this line, expressed as the classic rise over run formulation, was accurately calculated it would tell us by how much our dependant variable changed and by what direction *on average* when we vary our independent variables. Quantifying the linear relationship between an explanatory variable and an event of interest is, as you can imagine, of interest to policymakers, businesspeople, and nerds.

```{r,echo=FALSE, warning=FALSE}

d2 %>% 
  ggvis(~Ferbie_Sales, ~ Violent_Crime, stroke:="blue") %>%
  layer_points() %>%
  layer_model_predictions(model = "lm", stroke := "orange", fill := "orange")
```

How do we get that line, though, that that quantifies this relationship? OLS helps us pick the best line and determine the relationship by defining 'best' as a line that minimizes the average squared distance of the points in this stochastic process. However, note that the previous plot doesn't show us what origin (if any) to pass the line through ($\hat\beta_0$). 

This is the topic of today's formulation. The oft ignored and slightly maligned estimate for $\hat\beta_0$ does actually yield some interesting information. When I thonk of $\hat\beta_0$ by itself this i

It tells us, for example which is what would our dependent variable be if our explanatory variable was set to 0.  In other the above simulated dataset, it would answer the question: "what kind of utopia would we live in where Ferbie sales were noexistent and violent crime is but a memory?"" *Imagine*

Let's first explictly define error ($E$) in the manner previously mentioned:

$E = {(y_i - \hat\beta_0 - \hat\beta_1x_i)^2}$

We want to find the line that minimizes $E$ and get from this problem

  $\underset{\hat\beta_0 \hat\beta_1}{\text{min}}\displaystyle\sum_{i=1}^{n}{(y_i - \hat\beta_0 - \hat\beta_1x_i)^2}$


To this simple classic equation:

$\hat\beta_0$ = $\bar{y}$ - $\hat\beta_1 \bar{x}$

Note that in this minimization problem we only have the power to change the values of $\hat\beta_0$ and $\hat\beta_1$ This means, in other words, that the values we pick for $\hat\beta_0$ and $\hat\beta_1$ must be optimal over *all* values of $x_i$ and $y_i$

This begs the question - how will I know when I've minimized $E$ with respect to $\hat\beta_0$? Well, if a best solution exists, there should be a value of $\hat\beta_0$ that resides at a local minimum for the error $E$. With the right value for the Beta slope of the error is flat. This what is known as minimizing the residual sum of squares.

If $\hat\beta_0$ and$\hat\beta_1$ are chosen correctly then any perturbation away from that choice will have an error slope of zero.

Suppose that have some function and we've initially guessed on the following values for our parameters:

$\hat\beta_0 = 2$

$\hat\beta_1 = 3$

And that the error from these choices is:

$E(2,3) = 5$

Then, we nudge $\hat\beta_0$ a little bit and see what happens to the error:

$\hat\beta_0 = 2.1$

$\hat\beta_1 = 3$

$E(2.1,3) = 5.1$

Small increase. Ok. But then what if we nudge $\hat\beta_0$ *the other direction* and get the following:

$\hat\beta_0 = 1.9$

$\hat\beta_1 = 3$

$E(1.9,3) = 4.9$

Now  this symmetrical decrease is a problem. It means that we have not reached a local minimum. In this hypothetical scenario the error went down symetrically more or less and therefore the values of$\hat\beta_0$ and $\hat\beta_1$ are  not on the lowest, flattest point of the error curve. What we are going for is a situation where the change is small relative to the input change by *some order of magnitude*.

Picture a marble running up and down the sides of a bowl. When you first let it go it swings up and down the sides of the bowl but eventually starts to settle. That is where you want  your error fluctations to be. That is where you have reached your local mimima. This is the point of minimized sum of squared residuals.

Today we are looking for the value of $\hat\beta_0$ that achieves this, but there is that pesky problem of the fact that our argument requires us take into account both $\hat\beta_0$ and $\hat\beta_1$ No worries, multivariate calculus will help us do that.

To tackle this let us set the partial derivatives of $E$ with respect to the betas to 0 so that we can measure the immediate comparable change when we nudge one the betas and hold the other steady.

$\frac{\partial E}{\partial \hat\beta_1} = 0$


$\frac{\partial E}{\partial \hat\beta_0} = 0$

Using the sum rule form of distribution, the derivative can be moved inside:

 $\displaystyle\sum_{i=1}^{n}\frac{\partial }{\partial \hat\beta_0} {(y_i - \hat\beta_0 - \hat\beta_1x_i)^2 }= 0$

Then apply the chain rule to get:

$\displaystyle\sum_{i=1}^{n}2{(y_i - \hat\beta_0 - \hat\beta_1x_i)} (-1)$

and set to 0 :

$\displaystyle\sum_{i=1}^{n}2{(y_i - \hat\beta_0 - \hat\beta_1x_i)} (-1) = 0$

Next factor out the 2

$2\displaystyle\sum_{i=1}^{n}{(y_i - \hat\beta_0 - \hat\beta_1x_i)} (-1) = 0$

Factorout the $-1$ and divide both sides by $2$. Finally, also divide by $-1$ to simplify 

$\displaystyle\sum_{i=1}^{n}{(y_i - \hat\beta_0 - \hat\beta_1x_i)}  = 0$

Again, this is saying that the derivative of the sum of squared residuals with respect to $\hat\beta_0$ should add to 0. From here we distribute the summation operator across the sum as such:


$\displaystyle\sum_{i=1}^{n}{y_i - \sum_{i=1}^{n}\hat\beta_0 - \sum_{i=1}^{n}\hat\beta_1x_i }  = 0$


Things start to move pretty quickly here. 

For starters, see that the sum of y from to the n is the same thing as N times the average of $y$. Because the sum of our $\hat\beta_0$  is simply a constant repeating over and over, it is the same thing as $N \cdot \hat\beta_0$. Finally in the this step, we can factor out the  $\hat\beta_1x_i$ and move it to the left of the summation operator :

$N\bar y + N(-\hat\beta_0)  -\hat\beta_1\displaystyle\sum_{i=1}^{n}x_i$

Now that the -$\hat\beta_1$ is outside the $\displaystyle\sum_{i=1}^{n}x_i$, the latter can also be re-written can be written as $N\bar x$


$N\bar y + N(-\hat\beta_0)  -\hat\beta_1\ N\bar x$

We can factor $N$ because of the distributive law:

$N(\bar y - \hat\beta_0  -\hat\beta_1\ \bar x)$

Divide by $N$ to get rid of it

Now we have the pentultimate step:

$\bar y - \hat\beta_0  -\hat\beta_1\ \bar x = 0$

Finally, solve for $\hat\beta_0$

$\bar y   -\hat\beta_1\ \bar x = \hat\beta_0$

And there we have it, now we know where to put the $y$ intercept.

In the next post, I will tackle the more difficult problem of using this newfound knowledge to derive the formula for $\hat\beta_1$